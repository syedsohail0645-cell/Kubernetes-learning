1ï¸âƒ£ What is a Namespace in Kubernetes?

A Namespace is a logical partition inside a Kubernetes cluster used to group and isolate resources.

Think of a namespace like a folder inside a cluster.

Cluster
 â”œâ”€â”€ default
 â”œâ”€â”€ kube-system
 â”œâ”€â”€ dev
 â”œâ”€â”€ qa
 â””â”€â”€ prod

2ï¸âƒ£ Why do we need Namespaces? (Core Problem)

Without namespaces:

All Pods, Services, Deployments live together

Name conflicts happen

No isolation

Hard to manage access & limits

âŒ Example problem:

Pod name: web
Pod name: web  â† conflict!

3ï¸âƒ£ What problems Namespaces solve âœ…
ğŸ”¹ 1. Resource Isolation

You can separate workloads:

Namespace	Purpose
dev	Development
test	Testing
prod	Production

Same resource names allowed:

dev/web
prod/web

ğŸ”¹ 2. Access Control (RBAC)

You can say:

Dev team â†’ only dev namespace

Ops team â†’ all namespaces

Example:

User A â†’ dev namespace only
User B â†’ prod namespace only

ğŸ”¹ 3. Resource Quotas & Limits

Namespaces allow:

CPU limits

Memory limits

Pod count limits

Example:

dev namespace â†’ max 5 Pods
prod namespace â†’ max 50 Pods


Prevents resource abuse.

ğŸ”¹ 4. Easier Management

You can:

List resources per namespace

Delete all resources in one namespace

Apply policies per namespace

Example:

kubectl delete ns dev


â¡ Deletes everything inside dev.

4ï¸âƒ£ Built-in Namespaces (VERY IMPORTANT)

Run:

kubectl get ns


Youâ€™ll see:

Namespace	Purpose
default	Default user workloads
kube-system	Control plane components
kube-public	Public readable data
kube-node-lease	Node heartbeat info
5ï¸âƒ£ Namespaced vs Cluster-Scoped Resources

âš ï¸ Very important distinction

Namespaced resources:

Pod

Service

Deployment

ConfigMap

Secret

Cluster-scoped resources:

Node

Namespace

PersistentVolume

ClusterRole

ğŸ“Œ Namespace itself is NOT namespaced.

6ï¸âƒ£ How Namespace works internally (API view)

Create namespace:

kubectl create ns dev


API request:

POST /api/v1/namespaces


Get pods in namespace:

kubectl get pods -n dev


API request:

GET /api/v1/namespaces/dev/pods

7ï¸âƒ£ YAML Example
apiVersion: v1
kind: Namespace
metadata:
  name: dev


Create:

kubectl apply -f namespace.yaml

8ï¸âƒ£ Default Namespace behavior

If you donâ€™t specify namespace:

Kubernetes uses default

Example:

kubectl get pods


Same as:

kubectl get pods -n default

9ï¸âƒ£ One Golden Rule ğŸ§ ğŸ”¥
Namespace = logical isolation
Cluster = physical infrastructure

ğŸ”Ÿ Interview-Ready Answer (2â€“3 lines)

A namespace is a logical isolation mechanism in Kubernetes used to organize and separate resources within a cluster.
It helps with access control, resource quotas, and managing multiple environments like dev, test, and prod.


======================================================================================================================================================================================================================

LABLES
========

1ï¸âƒ£ What are Labels in Kubernetes? (First principles)

Labels are simple keyâ€“value pairs attached to Kubernetes objects to identify and group them.

Example:

labels:
  app: hello-world
  tier: prod


They are:

Not unique

Not meant for metadata storage

Meant for selection & grouping

Why labels exist (THE real reason)

Kubernetes never hard-codes relationships.

Instead:

Services find Pods using labels

ReplicaSets manage Pods using labels

kubectl selects resources using labels

Schedulers can use labels

ğŸ‘‰ Labels are the glue that connects Kubernetes objects.

2ï¸âƒ£ Mental Model (remember forever ğŸ§ )
Labels answer: â€œWHAT is this?â€
Selectors answer: â€œWHICH ones do I want?â€

3ï¸âƒ£ First Demo: Pods with Labels
Create Pods with labels
kubectl apply -f CreatePodsWithLabels.yaml


This YAML creates multiple Pods, each with labels like:

tier: prod
tier: qa
app: MyWebApp


ğŸ‘‰ Labels are attached at creation time.

Show labels on all Pods
kubectl get pods --show-labels


Shows:

nginx-pod-1   tier=prod,app=MyWebApp
nginx-pod-2   tier=qa,app=MyWebApp


ğŸ‘‰ Confirms labels are just metadata

View labels of a single Pod
kubectl describe pod nginx-pod-1 | head


Youâ€™ll see:

Labels: tier=prod

4ï¸âƒ£ Querying Pods using Labels (Selectors)
Select Pods with tier=prod
kubectl get pods --selector tier=prod


or

kubectl get pods -l tier=prod


ğŸ‘‰ Both are identical.

Multiple label selectors
kubectl get pods -l 'tier=prod,app=MyWebApp'


Meaning:

tier=prod AND app=MyWebApp

Not equal selector
kubectl get pods -l 'tier=prod,app!=MyWebApp'

Set-based selectors
kubectl get pods -l 'tier in (prod,qa)'
kubectl get pods -l 'tier notin (prod,qa)'


ğŸ‘‰ Very powerful for bulk operations.

5ï¸âƒ£ Showing Labels as Columns
kubectl get pods -L tier
kubectl get pods -L tier,app


Adds label values as columns:

NAME          TIER   APP
nginx-pod-1  prod   MyWebApp

6ï¸âƒ£ Editing Labels (IMPORTANT)
Update existing label
kubectl label pod nginx-pod-1 tier=non-prod --overwrite


ğŸ‘‰ Labels are mutable.

Add a new label
kubectl label pod nginx-pod-1 another=Label

Remove a label
kubectl label pod nginx-pod-1 another-


Dash (-) means delete label.

Bulk label operation
kubectl label pod --all tier=non-prod --overwrite


ğŸ‘‰ Labels enable mass operations.

Delete Pods by label
kubectl delete pod -l tier=non-prod


ğŸ’¥ Deletes all matching Pods.

7ï¸âƒ£ Labels + Controllers (Deployment, ReplicaSet, Pods)
Create Deployment
kubectl apply -f deployment-label.yaml


Deployment defines:

selector:
  matchLabels:
    app: hello-world

What happens internally
Deployment
   â†“ (selector)
ReplicaSet
   â†“ (labels)
Pods


The ReplicaSet only manages Pods matching its selector.

Describe Deployment
kubectl describe deployment hello-world


Youâ€™ll see:

Selector: app=hello-world

Describe ReplicaSet
kubectl describe replicaset hello-world


ReplicaSet adds:

pod-template-hash


This prevents old/new Pods mixing.

8ï¸âƒ£ Breaking ReplicaSet on purpose (VERY IMPORTANT DEMO)
Change Pod label manually
kubectl label pod POD_NAME pod-template-hash=DEBUG --overwrite


What happens:

ReplicaSet no longer recognizes this Pod

It creates a new Pod

Old Pod still exists

ğŸ‘‰ Controllers rely only on labels, not Pod names.

9ï¸âƒ£ Labels + Services (LOAD BALANCING)
Service selector
kubectl describe service hello-world


Selector:

app=hello-world


Service routes traffic to all Pods matching selector.

Endpoints
kubectl describe endpoints hello-world


Shows IPs of Pods selected by Service.

Remove Pod from load balancing
kubectl label pod POD_NAME app=DEBUG --overwrite


Result:

Pod still running

Service stops sending traffic

ReplicaSet creates a replacement Pod

ğŸ”¥ This proves:

Services donâ€™t know Pods â€” they know labels

1ï¸âƒ£0ï¸âƒ£ Labels + Scheduling (Nodes)
Show node labels
kubectl get nodes --show-labels

Add labels to nodes
kubectl label node c1-node2 disk=local_ssd
kubectl label node c1-node3 hardware=local_gpu

Verify
kubectl get node -L disk,hardware

Pods using nodeSelector
kubectl apply -f PodsToNodes.yaml


Example:

nodeSelector:
  disk: local_ssd


ğŸ‘‰ Scheduler places Pod only on matching nodes.

Confirm placement
kubectl get pods -o wide


Shows which node each Pod landed on.

1ï¸âƒ£1ï¸âƒ£ Clean-up

Removing labels and Pods resets everything.
